# -*- coding: utf-8 -*-
"""CSE258_Hw4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lBL5IdpH_mvIITfe-3x1NMLcelzCd_zf
"""

import gzip
import math
import numpy
import random
import sklearn
import string
from collections import defaultdict
from gensim.models import Word2Vec
from nltk.stem.porter import *
from sklearn import linear_model
from sklearn.manifold import TSNE
import dateutil

import warnings
warnings.filterwarnings("ignore")
def assertFloat(x):
    assert type(float(x)) == float

def assertFloatList(items, N):
    assert len(items) == N
    assert [type(float(x)) for x in items] == [float]*N

"""# Importing Data"""

dataset = []

f = gzip.open("young_adult_20000.json.gz")
for l in f:
    d = eval(l)
    dataset.append(d)
    if len(dataset) >= 20000:
        break
        
f.close()

answers = {}

"""# Text Mining

## Q1

### Unigram
"""

dataset[0].keys()

trainData = dataset[:10000]
testData = dataset[10000:]

# dataset[:1]
wordCount = defaultdict(int)
punctuation = set(string.punctuation)
for d in trainData:
  r = ''.join([c for c in d['review_text'].lower() if not c in punctuation])
  for w in r.split():
    wordCount[w] += 1

counts = [(wordCount[w], w) for w in wordCount]
counts.sort()
counts.reverse()

print(counts)

words = [x[1] for x in counts[:1000]]
print(words)

X = []
y = []
for i in trainData:
  dick = [0]*(len(words))
  # print(dick)
  r = ''.join([c for c in i['review_text'].lower() if not c in punctuation])
  for word in r.split():
    # print(word)
    if word in words:
      dick[words.index(word)] += 1
      # print(word)
  # print("bruh")
  # break
  xtmp = [1]
  xtmp.extend(dick)
  X.append(xtmp)
  y.append(i["rating"])

print(X[0])

# print(sum(X[1]))

# Regularized regression
clf = linear_model.Ridge(1.0, fit_intercept=False) # MSE + 1.0 l2
clf.fit(X, y)
theta = clf.coef_

X_test = []
for i in testData:
  dick = [0]*(len(words))
  # print(dick)
  r = ''.join([c for c in i['review_text'].lower() if not c in punctuation])
  for word in r.split():
    # print(word)
    if word in words:
      dick[words.index(word)] += 1
      # print(word)
  # print("bruh")
  # break
  xtmp = [1]
  xtmp.extend(dick)
  X_test.append(xtmp)

predictions = clf.predict(X_test)

y[0]

wordSort = list(zip(theta[:-1], words))
wordSort.sort()

print(wordSort)

preMse = 0
for i in range(len(X_test)):
  preMse+= (predictions[i]-float(testData[i]["rating"]))**2
MSE = preMse/len(X_test)

print(MSE)

answers["Q1a"] = [float(MSE), [x[1] for x in wordSort[:5]], [x[1] for x in wordSort[-5:]]]

answers



"""### Bigram"""

dataset[0].keys()

# dataset[:1]
wordCount = defaultdict(int)
punctuation = set(string.punctuation)
for d in trainData:
  r = ''.join([c for c in d['review_text'].lower() if not c in punctuation])
  tmp = r.split()
  for w in range(len(tmp)-1):
    wordCount[tmp[w]+" "+tmp[w+1]] += 1

counts = [(wordCount[w], w) for w in wordCount]
counts.sort()
counts.reverse()

print(counts[:2])

words = [x[1] for x in counts[:1000]]
print(words[0])

X = []
y = []
for i in trainData:
  dick = [0]*(len(words))
  # print(dick)
  r = ''.join([c for c in i['review_text'].lower() if not c in punctuation])
  tmp = r.split()
  for word in range(len(tmp)-1):
    biwrd = tmp[word]+" "+tmp[word+1]
    if biwrd in words:
      dick[words.index(biwrd)] += 1
      # print(word)
  # print("bruh")
  # break
  xtmp = [1]
  xtmp.extend(dick)
  X.append(xtmp)
  y.append(i["rating"])

print(sum(X[0]))

# Regularized regression
clf = linear_model.Ridge(1.0, fit_intercept=False) # MSE + 1.0 l2
clf.fit(X, y)
theta = clf.coef_

X_test = []
for i in testData:
  dick = [0]*(len(words))
  # print(dick)
  r = ''.join([c for c in i['review_text'].lower() if not c in punctuation])
  tmp = r.split()
  for word in range(len(tmp)-1):
    biwrd = tmp[word]+" "+tmp[word+1]
    if biwrd in words:
      dick[words.index(biwrd)] += 1
      # print(word)
  # print("bruh")
  # break
  xtmp = [1]
  xtmp.extend(dick)
  X_test.append(xtmp)



predictions = clf.predict(X_test)

wordSort = list(zip(theta[:-1], words))
wordSort.sort()

print(wordSort)

preMse = 0
for i in range(len(X_test)):
  preMse+= (predictions[i]-testData[i]["rating"])**2
MSE = preMse/len(X_test)

print(MSE)

answers["Q1b"] = [float(MSE), [x[1] for x in wordSort[:5]], [x[1] for x in wordSort[-5:]]]

answers

"""### Combi Uni and Bi"""



dataset[0].keys()

# dataset[:1]
wordCount = defaultdict(int)
punctuation = set(string.punctuation)
for d in trainData:
  r = ''.join([c for c in d['review_text'].lower() if not c in punctuation])
  tmp = r.split()
  if len(tmp)>0:
    wordCount[tmp[0]] +=1
  for w in range(len(tmp)-1):
    wordCount[tmp[w]+" "+tmp[w+1]] += 1 # bigram
    wordCount[tmp[w+1]] +=1 # unigram

counts = [(wordCount[w], w) for w in wordCount]
counts.sort()
counts.reverse()

print(counts[:2])

words = [x[1] for x in counts[:1000]]
print(words[0])

words[:10]

X = []
y = []
for i in trainData:
  dick = [0]*(len(words))
  # print(dick)
  r = ''.join([c for c in i['review_text'].lower() if not c in punctuation])
  tmp = r.split()
  if len(tmp)>0:
    if tmp[0] in words:
      dick[words.index(tmp[0])] += 1
  for word in range(len(tmp)-1):
    biwrd = tmp[word]+" "+tmp[word+1]
    if biwrd in words:
      dick[words.index(biwrd)] += 1
    if tmp[word+1] in words:
      dick[words.index(tmp[word+1])] += 1
      # print(word)
  # print("bruh")
  # break
  xtmp = [1]
  xtmp.extend(dick)
  X.append(xtmp)
  y.append(i["rating"])

print(sum(X[0]))

# Regularized regression
clf = linear_model.Ridge(1.0, fit_intercept=False) # MSE + 1.0 l2
clf.fit(X, y)
theta = clf.coef_

X_test = []
for i in testData:
  dick = [0]*(len(words))
  # print(dick)
  r = ''.join([c for c in i['review_text'].lower() if not c in punctuation])
  tmp = r.split()
  if len(tmp)>0:
    if tmp[0] in words:
      dick[words.index(tmp[0])] += 1
  for word in range(len(tmp)-1):
    biwrd = tmp[word]+" "+tmp[word+1]
    if biwrd in words:
      dick[words.index(biwrd)] += 1
    if tmp[word+1] in words:
      dick[words.index(tmp[word+1])] += 1
      # print(word)
  # print("bruh")
  # break
  xtmp = [1]
  xtmp.extend(dick)
  X_test.append(xtmp)



predictions = clf.predict(X_test)

wordSort = list(zip(theta[:-1], words))
wordSort.sort()

print(wordSort)

preMse = 0
for i in range(len(X_test)):
  preMse+= (predictions[i]-testData[i]["rating"])**2
MSE = preMse/len(X_test)

print(MSE)

answers["Q1c"] = [float(MSE), [x[1] for x in wordSort[:5]], [x[1] for x in wordSort[-5:]]]

answers

for q in 'Q1a', 'Q1b', 'Q1c':
    assert len(answers[q]) == 3
    assertFloat(answers[q][0])
    assert [type(x) for x in answers[q][1]] == [str]*5
    assert [type(x) for x in answers[q][2]] == [str]*5

"""## Q2"""

# dataset[:1]
wordCount = defaultdict(int)
punctuation = set(string.punctuation)
for d in trainData:
  r = ''.join([c for c in d['review_text'].lower() if not c in punctuation])
  for w in r.split():
    wordCount[w] += 1

counts = [(wordCount[w], w) for w in wordCount]
counts.sort()
counts.reverse()
print(counts)
words = [x[1] for x in counts[:1000]]
print(words)

# doc freq
docFreq = defaultdict(int) # Cnt if how many doc has a word
for i in trainData:
  r = ''.join([c for c in i['review_text'].lower() if not c in punctuation])
  for word in r.split():
    docFreq[word] += 1

# termFreq of everything
allDoctf = []
for i in trainData:
  termFreq = defaultdict(int)
  r = ''.join([c for c in i['review_text'].lower() if not c in punctuation])
  for word in r.split():
    termFreq[word] += 1
  allDoctf.append(termFreq)

def Cosine(x1,x2):
    numer = 0
    norm1 = 0
    norm2 = 0
    for a1,a2 in zip(x1,x2):
        numer += a1*a2
        norm1 += a1**2
        norm2 += a2**2
    if norm1*norm2:
        return numer / math.sqrt(norm1*norm2)
    return 0

# Calculating tfidf for the top 1000 common words in the first document.
tfidf0 = dict(zip(words,[allDoctf[0][w] * math.log2(len(dataset) / docFreq[w]) for w in words]))
tfidfQuery0 = [allDoctf[0][w] * math.log2(len(dataset) / docFreq[w]) for w in words]

maxTf = [(allDoctf[0][w],w) for w in words]
maxTf.sort(reverse=True)
maxTfIdf = [(tfidf0[w],w) for w in words]
maxTfIdf.sort(reverse=True)

maxTf[:10]

cosiL = []
for i in range(1,len(trainData)):
  tfidf = [allDoctf[i][w] * math.log2(len(dataset) / docFreq[w]) for w in words]
  cosiL.append((Cosine(tfidfQuery0,tfidf),trainData[i]["review_text"]))

cosiL.sort(reverse=True)

cosiL[0]

answers['Q2'] = [cosiL[0][0], cosiL[0][1]]
assert len(answers['Q2']) == 2
assertFloat(answers['Q2'][0])
assert type(answers['Q2'][1]) == str

answers

"""# Content Struc Sequences

## Q3
"""

reviewsPerUser = defaultdict(list)

dataset[0].keys()

for d in dataset:
    reviewsPerUser[d['user_id']].append((dateutil.parser.parse(d['date_added']), d['book_id']))

len(reviewsPerUser)

reviewLists = []
for u in reviewsPerUser:
    rl = list(reviewsPerUser[u])
    rl.sort()
    reviewLists.append([x[1] for x in rl])
print(reviewLists)

model10 = Word2Vec(reviewLists,
                 min_count=1, # Words/items with fewer instances are discarded
                 size=10, # Model dimensionality
                 window=3, # Window size
                 sg=1) # Skip-gram model

mainWord = dataset[0]["book_id"]

mainWord, len(reviewLists)

model10.wv.similar_by_word(mainWord)

answers['Q3'] = model10.wv.similar_by_word(mainWord)[:5]
assert len(answers['Q3']) == 5
assert [type(x[0]) for x in answers['Q3']] == [str]*5
assertFloatList([x[1] for x in answers['Q3']], 5)

answers

"""## Q4"""

dataset = []

f = gzip.open("young_adult_20000.json.gz")
for l in f:
    d = eval(l)
    dataset.append(d)
    if len(dataset) >= 20000:
        break
        
f.close()

dataset[0].keys()

# dataTrain = []
# for i in dataset:
#   tmp = [i["user_id"],i["book_id"],i["rating"]]
#   dataTrain.append(tmp)

# Some data structures you might want
usersPerItem = defaultdict(set) # Maps an item to the users who rated it
itemsPerUser = defaultdict(set) # Maps a user to the items that they rated
reviewsPerUser = defaultdict(list)
reviewsPerItem = defaultdict(list)
ratingDict = {} # To retrieve a rating for a specific user/item pair

from tqdm.notebook import tqdm
import time

for i in dataset:
  itemsPerUser[i['user_id']].add(i['book_id'])
  
  usersPerItem[i['book_id']].add(i['user_id'])


  reviewsPerUser[i['user_id']].append(i['rating'])

  reviewsPerItem[i['book_id']].append(i['rating'])

  ratingDict[(i["user_id"],i["book_id"])] = i['rating']

  # print(itemsPerUser)

globalAvg = sum(ratingDict.values())/len(ratingDict)

def Cosine(x1,x2):
    numer = 0
    norm1 = 0
    norm2 = 0
    for a1,a2 in zip(x1,x2):
        numer += a1*a2
        norm1 += a1**2
        norm2 += a2**2
    if norm1*norm2:
        return numer / math.sqrt(norm1*norm2)
    return 0

def avgItemReview(item):
  try:
    return sum(reviewsPerItem[item])/len(reviewsPerItem[item])
  except:
    return 0

def ratingPred(user,item):
  if item not in usersPerItem:
    print("no user")
    return globalAvg
  if user not in itemsPerUser:
    print("no item")
    return globalAvg
  itemsInteracted = itemsPerUser[user] # items given user
  numi = 0
  deno = 0
  # print(len(reviewsPerUser[user]))
  # print(len(itemsPerUser[user]))
  cosi = []
  for i in itemsInteracted:
    if i == item:
      # print(i,item)
      # print("bruh")
      continue
    else:
      # print(model10.wv.distance(i,item))
      try:
        numi += ((ratingDict[(user,i)]-avgItemReview(i)) * model10.wv.distance(i,item))
        deno += model10.wv.distance(i,item)
      except:
        tmp = avgItemReview(item)
        if tmp == 0:
          return globalAvg
        else:
          return tmp

      # cosi.append(Cosine(model10.wv[i],model10.wv[item]))
      # print(Cosine(model10.wv[i],model10.wv[item]))

  if numi == 0:
    # print("no user")
    return avgItemReview(item) + 0
  if deno == 0:
    # print("bruh")
    return avgItemReview(item) + 0
  # print("1")
  # if avgItemReview(item)+(numi/deno) < 0:
  #   print(avgItemReview(item))
  #   print(cosi)
  #   print(numi/deno)
  return avgItemReview(item)+(numi/deno)

PreMse=0
dataTest = dataset[:1000]
PrED = []
for i in dataTest:
  user = i['user_id']
  item = i["book_id"]
  y = i["rating"]
  # print(y)
  y_pred = ratingPred(user,item)
  # if y_pred<0:
  #   y_pred = 0
  # elif y_pred>5:
  #   y_pred = 5
  PrED.append(y_pred)
  # print(y_pred)
  
  PreMse += (y-y_pred)**2
  # print(y,y_pred)
  # break
  # reviewsPerUser[i['user_id']].append(i['rating'])

MSE = PreMse/len(dataTest)
print(MSE,globalAvg)

answers["Q4"] = MSE
answers

model10.wv[item]

ratingDict

globalAvg

model10 = Word2Vec(reviewLists,
                 min_count=1, # Words/items with fewer instances are discarded
                 size=20, # Model dimensionality
                 window=4, # Window size
                 sg=1) # Skip-gram model

PreMse=0
dataTest = dataset[:1000]
PrED = []
for i in dataTest:
  user = i['user_id']
  item = i["book_id"]
  y = i["rating"]
  # print(y)
  y_pred = ratingPred(user,item)
  if y_pred<0:
    y_pred = 0
  elif y_pred>5:
    y_pred = 5
  PrED.append(y_pred)
  # print(y_pred)
  
  PreMse += (y-y_pred)**2
  # print(y,y_pred)
  # break
  # reviewsPerUser[i['user_id']].append(i['rating'])

MSE = PreMse/len(dataTest)
print(MSE,globalAvg)

answers["Q5"] = ["description of your solution",
                 MSE]
answers

